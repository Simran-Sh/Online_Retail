## InfoTact Solutions Project

## Team - Data Analytics G2

---

## Project 1 (Month 1 Jan 2026): 
Building a Customer Intelligence Pipeline

---

## PROJECT OBJECTIVE
Built a fully automated, production-grade customer intelligence pipeline using SQL, Python, and Power BI, implementing RFM segmentation, cohort retention analysis, and market basket modeling to proactively identify churn risk customers and high-value whales

---

## Flow (end-to-end):

Raw Kaggle CSV <br>
   â†“  <br>
Data Cleaning & Modeling (Python)  <br>
   â†“  <br>
Star Schema in SQL (Fact + Dimensions)  <br>
   â†“  <br>
Single Customer View (SQL View)  <br>
   â†“  <br>
Advanced Analytics (Python: RFM, Cohorts, Market Basket)  <br>
   â†“  <br>
Power BI Dashboards (with RLS)  <br>
   â†“  <br>
Automation & Executive Output  <br>

---

## ðŸ§° Tools Stack (Industry-Standard)
| Layer         | Tool                            | Why                    |
| ------------- | ------------------------------- | ---------------------- |
| Storage       | SQL Server (SSMS 22)            | Scalable analytics DB  |
| ETL           | Python (pandas, SQLAlchemy)     | Robust transformations |
| Analytics     | Python (NumPy, mlxtend)         | RFM, Cohorts, Apriori  |
| Visualization | Power BI                        | Enterprise dashboards  |
| Automation    | Task Scheduler / Cron           | Hands-free pipeline    |

---

# WEEK 1 â€” Data Engineering & Schema Design

ðŸŽ¯ Goal
Convert raw transactional logs into a clean, trusted, analytics-ready data with an optimized star schema and create a Single Customer View that downstream analytics can trust and query in < 2 seconds.

---

## 1ï¸âƒ£ Understand the Raw Data 

## Key issues we must handle:
âŒ Negative Quantity â†’ returns
âŒ Negative Price â†’ data errors
âŒ Missing Customer ID i.e found "2,43,007"
âŒ InvoiceDate as string
âŒ Duplicate invoices

## 2ï¸âƒ£ Data Cleaning (Python â€“ pandas)
- **Handle Missing Customer IDs**
   Removed rows without Customer ID (cannot do RFM) and also Converted float â†’ int for speed

- **Handle Returns & Invalid Data**
   Filtering out returns & corrupt rows for small table and faster aggregations

- **Create Revenue**
   Creating derived metric i.e Monetary (M in RFM), leads to faster SQL queries

- **Convert dates**
  Converts string â†’ datetime for Recency calculation, Time-based indexing, Partitioning, otherwise SQL queries become slow and RFM Recency becomes impossible

---

## 3ï¸âƒ£ Load Data into SQL (ETL â†’ L)

# ðŸ”§ Tools
- SQL 
- SQLAlchemy


# ðŸ”ŒPython â†’ SQL Connection
SQLAlchemy Creates DB connection and handles Transactions, Data typing, Bulk inserts faster than row-by-row inserts

**Points to Note:**
   - Even though SSMS works, Python does NOT automatically inherit SSMS drivers. Hence import pyodbc and call **pyodbc.drivers()**
   - SSMS bundles its own drivers and Python relies on system ODBC registry
   - SQLAlchemy wraps pyodbc and Root cause is always lower-level ODBC
   - This matters in production, since CI/CD servers often donâ€™t have drivers

# Load Fact Table into SQL
Used Pandas "to_SQL" function to load cleaned data into SQL with "chunksize" method to prevent memory crashes and "replace" to ensure idempotency. 
Here for performance perpective, the Chunked inserts = stable loads

---

## 4ï¸âƒ£ Star Schema Design

Followed a pragmatic **star-schema design** Country was modeled as a customer attribute because it had no independent hierarchies, and Date attributes were derived dynamically since the use case didnâ€™t require a full calendar dimension.â€

                Dim_Customer (CustomerID, Country)
                     |
Dim_Product â€”â€” Fact_Sales 
                     

# ðŸ“¦ Fact Table: fact_sales
| Column       |
| ------------ |
| invoice_no   |
| invoice_date |
| customer_id  |
| stock_code   |
| quantity     |
| revenue      |

# ðŸ‘¤ Dim Customer
| Column      |
| ----------- |
| customer_id |
| country     |

# ðŸ›’ Dim Product
| Column      |
| ----------- |
| stock_code  |
| description |

---

## 5ï¸âƒ£ Indexing
Created Indexes (Clustered and Non Clustered) to speedup joins and filters and for RFM and Cohorts rely on date and customer
This indexing will specifically reduce the query time from seconds to milliseconds. Without indexes, Full table scans will take place

| Index       | Enables             |
| ----------- | ------------------- |
| CustomerID  | RFM, churn analysis |
| InvoiceDate | Cohorts, recency    |
| Invoice     | Frequency accuracy  |

---

## 6ï¸âƒ£ Single Customer View (CRITICAL)
View creates a Logical abstraction with No data duplication

## ðŸ”¥ Customer 360 View
1ï¸âƒ£ Single Source of Truth per Customer
   - One row per customer
   - Eliminates duplicate or conflicting customer metrics
   - Ensures all teams use the same definitions for frequency, spend, and activity

2ï¸âƒ£ Foundation for RFM & Churn Analysis
   - Frequency â†’ number of unique purchase events
   - Monetary â†’ total revenue generated
   - LastPurchaseDate â†’ anchor for recency calculation

ðŸ‘‰ These raw metrics are stable, auditable, and reusable

3ï¸âƒ£ Performance-Optimized for Analytics & Dashboards
   - Built on indexed fact tables
   - Lightweight aggregations in SQL
   - Fast enough for:
      - Python analytics
      - Power BI dashboards
      - Automated pipelines

---

# âœ… Week 1 Deliverables
âœ” Cleaned raw transactional data
âœ” Controlled ETL into SQL Server
âœ” Star schema
âœ” ER diagram
âœ” Proper fact table with correct grain
âœ” Proper dimension tables with enforced keys
âœ” Performance-ready indexes
âœ” Customer 360 View (single source of truth) â­

Week 1 TakeAway Points
- Debugged and resolved ODBC driver-level connection failures between Python and SQL Server Express by validating driver availability, aligning SQLAlchemy connection strings, and enforcing encrypted Windows authentication.â€

| Operation | Speed         | Logged     | Keeps Structure|
| --------- | ------------- | ---------  | ---------------|
| DELETE    | âŒ Slow      | âœ… Yes     | âœ… Yes         |
| TRUNCATE  | âš¡ Very Fast | âŒ Minimal | âœ… Yes         |
| DROP      | âš¡ Instant   | âŒ         | âŒ No          |

designed and implemented a production-ready star schema in SQL Server, enforced data integrity through constraints and window functions, and created a Customer 360 analytical view optimized for downstream RFM and churn analysis.

---

# WEEK 2 â€” Analytical Core (Python Intelligence - RFM & SEGMENTATION)

ðŸŽ¯ WEEK 2 BUSINESS GOAL
Build an RFM segmentation engine in Python on top of a SQL-based Customer 360 view, using quantile-based scoring and validating segment quality through revenue and recency distributions. Identify Whales (highest value customers) and Churn Risks in a repeatable, automated way, directly from SQL Server.

Input
âœ… dbo.vw_customer_360 (trusted, clean)

Output
âœ… Customer-level RFM table
âœ… Segments: Champions, Loyalists, At Risk, Hibernating
âœ… Validated with statistics (not vibes)

---

## STEP 1: CONNECT PYTHON TO SQL SERVER (READ-ONLY)

## STEP 2 â€” LOAD CUSTOMER 360 INTO PYTHON

## 1ï¸âƒ£ RFM Calculation

ðŸ§  Business Logic
| Metric    | Meaning                  |
| --------- | ------------------------ |
| Recency   | Days since last purchase |
| Frequency | Number of invoices       |
| Monetary  | Total revenue            |

## 2ï¸âƒ£ RFM Scoring (1â€“5 Scale)

## 3ï¸âƒ£ Customer Segmentation (Business Mapping)

## COHORT ANALYSIS
A cohort is a group of customers who made their first-ever purchase in the same time period (month or quarter).
Weâ€™ve used monthly cohorts (industry standard)

âœ” Defined acquisition cohorts
âœ” Calculated month-by-month retention
âœ” Built cohort matrix (counts + %)
âœ” Created 3 high-impact charts:
1ï¸âƒ£ Retention Heatmap (most important)
2ï¸âƒ£ Retention decay curve (trend over time)
3ï¸âƒ£ Cohort Size Trend (context)
âœ” Ready for Power BI cohort heatmap

I performed cohort-based retention analysis by grouping customers by first purchase month and tracking monthly activity decay, enabling long-term churn and LTV insights.

## 4ï¸âƒ£ Market Basket Analysis (MBA)
MBA works at invoice (basket) level, not customer level.
**ðŸŽ¯ Basket grain**
1 row = 1 invoice
1 column = 1 product

---

For MARKET BASKET ANALYSIS, We need:
Invoice (basket)
Product (Description)
Quantity

So we join fact_sales â†’ dim_product because in a star schema, descriptive attributes like product names live in dimension tables. The fact table stores only keys and metrics for performance

***basket_binary - applymap function***

| Aspect     | Old (`applymap`)     | New (vectorized)      |
| ---------- | -------------------- | --------------------- |
| Speed      | âŒ Slow (Python loop) | âš¡ Very fast (C-level) |
| Memory     | âŒ Inefficient        | âœ… Efficient           |
| Pandas 2.x | âŒ Removed            | âœ… Supported           |
| Production | âŒ Not ideal          | âœ… Best practice       |

"Implemented Market Basket Analysis using a vectorized binary invoiceâ€“product matrix, ensuring compatibility with pandas 2.x and improving performance over deprecated applymap usage"

## APRIORI
This is good for small datasets, but for us it did Multiple full scans of data and hence failed
Apriori has exponential memory complexity, so I constrained the item universe using frequency thresholds and limited itemset length, enabling scalable Market Basket Analysis.â€

---


## âœ… Week 2 Deliverables
âœ” Recency computed correctly
âœ” Frequency & Monetary reused from SQL
âœ” RFM scores (1â€“5)
âœ” RFM engine
âœ” Segments validated
âœ” COHORT ANALYSIS
âœ” Market basket rules
âœ” Statistical proof (Champions highest LTV)

# WEEK 3 â€” Power BI Dashboard (Storytelling)
ðŸ” Row Level Security (RLS)

Goal is to build decision-ready dashboards that:
   - Explain customer behavior
   - Surface churn risks & whales
   - Allow regional managers to self-serve
   - Are fast, clean, and trustworthy

# WEEK 4 â€” Automation & Executive Handoff

## 1ï¸âƒ£ Automate ETL

## 2ï¸âƒ£ Presentation Deck

## 3ï¸âƒ£ Full Pipeline Test

âœ” Raw CSV â†’ SQL
âœ” SQL â†’ Python
âœ” Python â†’ Power BI
âœ” Auto refresh works